{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "private_outputs": true,
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers SentencePiece pandas datasets tokenizers"
   ],
   "metadata": {
    "id": "m8lJWixC67Xp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (4.24.0)\r\n",
      "Collecting SentencePiece\r\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m0m\r\n",
      "\u001B[?25hRequirement already satisfied: pandas in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (1.4.4)\r\n",
      "Requirement already satisfied: datasets in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (2.7.1)\r\n",
      "Requirement already satisfied: tokenizers in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (0.13.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: filelock in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.9.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: requests in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from transformers) (22.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2022.7)\r\n",
      "Requirement already satisfied: xxhash in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.1.0)\r\n",
      "Requirement already satisfied: dill<0.3.7 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\r\n",
      "Requirement already satisfied: multiprocess in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (10.0.1)\r\n",
      "Requirement already satisfied: responses<0.19 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.3)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.11.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.8.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.13)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\r\n",
      "Installing collected packages: SentencePiece\r\n",
      "Successfully installed SentencePiece-0.1.97\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mlHTuFJd18dx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer\n",
    "# from tokenizers.models import BPE\n",
    "# from tokenizers.trainers import BpeTrainer\n",
    "# from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "# trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# tokenizer.pre_tokenizer = Whitespace()\n",
    "# files = [r'/content/drive/MyDrive/Colab Notebooks/NLP_proj/data/train.labeled']\n",
    "# tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_train(filename):\n",
    "  with open(filename) as f:\n",
    "    lines = f.read()\n",
    "  eng_entries = []\n",
    "  ger_entries = []\n",
    "  for entry in lines.split(2*os.linesep):\n",
    "    en_ger_couple = entry.split(\"English:\\n\")\n",
    "    if len(en_ger_couple)<=1:\n",
    "      continue\n",
    "    ger, eng = en_ger_couple[0].replace('German:\\n', ''), en_ger_couple[1]\n",
    "    eng_entries.append(eng)\n",
    "    ger_entries.append(ger)\n",
    "  return {\"ger\":ger_entries, \"eng\":eng_entries}\n",
    "\n",
    "train_data_dict = preprocess_train(r'data/train.labeled')\n",
    "val_data_dict = preprocess_train(r'data/val.labeled')"
   ],
   "metadata": {
    "id": "P6DqvfbHQpiH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ],
   "metadata": {
    "id": "7xKvfV-bbqJq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "685ec51b65684df2b1cd51c50e3789d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db853d48c1a744258816f1b258ae7b4f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.mosa/opt/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8af74f7b532d4dcfa62e58d5526050f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_dict(train_data_dict)\n",
    "eval_ds = Dataset.from_dict(val_data_dict)"
   ],
   "metadata": {
    "id": "2-z264uMbwdC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "id": "Rcjk1GwDdFwH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ger': 'Was ist da so falsch gelaufen?\\nDie Wirtschaftskrise scheint die naheliegendste Erklärung zu sein, vielleicht zu naheliegend.\\n',\n 'eng': 'What has gone so wrong?\\nThe economic crisis seems to be the most obvious explanation, but perhaps too obvious.'}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "eval_ds[0]"
   ],
   "metadata": {
    "id": "nT2-5YGmdOpo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ger': 'Und weiterreichende Kürzungen wie die von der EU vorgeschlagenen – 20 Prozent unterhalb der Werte von 1990 innerhalb von zwölf Jahren – würden die globalen Temperaturen bis 2100 lediglich um ein Sechzigstel Grad Celsius (ein Dreißigstel Grad Fahrenheit) senken, und das bei Kosten von 10 Billionen Dollar.\\nFür jeden ausgegebenen Dollar hätten wir nur eine Wertschöpfung von vier Cent erreicht.\\n',\n 'eng': 'And deeper emissions cuts like those proposed by the European Union – 20% below 1990 levels within 12 years – would reduce global temperatures by only one-sixtieth of one degree Celsius (one-thirtieth of one degree Fahrenheit) by 2100, at a cost of $10 trillion.\\nFor every dollar spent, we would do just four cents worth of good.'}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# the following 2 hyperparameters are task-specific\n",
    "max_source_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# tokenize the input\n",
    "input_sequences = train_ds[\"eng\"]\n",
    "output_sequences = train_ds[\"ger\"]\n",
    "\n",
    "encoding = tokenizer(\n",
    "    input_sequences,\n",
    "    padding=\"longest\",\n",
    "    max_length=max_source_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "input_ids, attention_mask = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "# encode the targets\n",
    "target_encoding = tokenizer(\n",
    "    output_sequences,\n",
    "    padding=\"longest\",\n",
    "    max_length=max_target_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "labels = target_encoding.input_ids\n",
    "\n",
    "# replace padding token id's of the labels by -100 so it's ignored by the loss\n",
    "labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n"
   ],
   "metadata": {
    "id": "d1NwSrzRdkZS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2751,   229,   836,  ...,  -100,  -100,  -100],\n        [28508,   521,   107,  ...,  -100,  -100,  -100],\n        [ 9515, 15690,   266,  ...,  3870,   177,     1],\n        ...,\n        [ 2215,     6,   319,  ...,  -100,  -100,  -100],\n        [  316, 31661,  5754,  ...,  -100,  -100,  -100],\n        [  604,     5,   781,  ...,  -100,  -100,  -100]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# forward pass\n",
    "loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
    "loss.item()"
   ],
   "metadata": {
    "id": "V13NXVMNerp-",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-dXfLbvV6ru0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "target_encoding.input_ids"
   ],
   "metadata": {
    "id": "dJJ732YU-bbi",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "oxt0gxDqM-AW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}